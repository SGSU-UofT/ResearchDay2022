{
  "articles": [
    {
      "path": "abstracts.html",
      "title": "Abstracts",
      "description": "Below are the abstracts for the talks, in alphabetical order by speaker's last name.",
      "author": [],
      "contents": "\nSpencer Frei\nBenign Overfitting without Linearity\nDeep learning has revealed a number of surprising statistical and computational phenomena. We will focus on one such surprise: the possibility of ‘benign’ overfitting. Experiments with neural networks trained by gradient descent have revealed that they are capable of simultaneously (1) overfitting to datasets that have substantial amounts of random label noise and (2) generalizing well to unseen data. This behavior appears inconsistent with the intuition from classical statistics that the bias-variance tradeoff should prevent a model that overfits noisy training data from performing well on test data.\nIn this talk we investigate this phenomenon in two-layer neural networks trained by gradient descent on the logistic loss following random initialization. We assume the data comes from well-separated class-conditional log-concave distributions and allow for a constant fraction of the training labels to be corrupted by an adversary. We show that in this setting, neural networks indeed exhibit benign overfitting: despite the non-convex nature of the optimization problem, the empirical risk is driven to zero, overfitting the many noisy labels; and as opposed to the classical intuition, the networks simultaneously generalize near-optimally. In contrast to previous works on benign overfitting that require linear or kernel-based predictors, our analysis holds in a setting where both the model and learning dynamics are fundamentally nonlinear. Based on joint work with Niladri Chatterji and Peter Bartlett.\nYuri Saporito\nMultiscale modeling for option pricing and optimal execution\nFast mean reversion is a crucial modeling tool in many areas of science. In Quantitative Finance, it was firstly introduced to model volatility and its consequence to option pricing was studied by Jean-Pierre Fouque and his co-authors. In this talk, I will review this application and present a brand-new use to optimal execution. This last part is a joint work with JP Fouque and Sebastian Jaimungal.\nSabrina Sixta\nConvergence rate bounds for iterative random functions using one-shot coupling\nOne-shot coupling is a method of bounding the convergence rate between two copies of a Markov chain in total variation distance. The method is divided into two parts: the contraction phase, when the chains converge in expected distance and the coalescing phase, which occurs at the last iteration, when there is an attempt to couple. The method closely resembles the common random number technique used for simulation. We present a general theorem for finding the upper bound on the Markov chain convergence rate that uses the one-shot coupling method. We then apply the general theorem to two families of Markov chains: the random functional autoregressive process and the randomly scaled iterated random function. Multiple examples will be presented to show how the theorem can be used on various models including ones in high dimensions.\nPreprint: https://arxiv.org/abs/2112.03982\nSpark Tseung\nModelling Heterogeneous Risks with Random Effects in the Mixture-of-Experts Model\nIn statistical applications, mixed (or random effects) models are often used for modelling unobserved effects or correlation across repeated measurements. In the class of models with random effects, linear mixed models (LMM) and generalized linear mixed models (GLMM) are well studied and widely applied in many modelling problems. However, several restrictive assumptions of these models have rendered them unsuitable for insurance data, which typically exhibit multimodality and rather different characteristics in the body and the tail of the distribution. In this talk, we present an extension to a class of the mixture-of-experts model proposed in Fung et al. (2019) by incorporating random effects. This non-trivial extension preserves the desirable property of denseness, i.e. the flexibility to capture any distributional shape, dependence and regression structure, while the intuitive model structure allows for mathematical tractability and interpretability. Besides, the addition of random effects accounts for unobserved effects such as heterogeneous risks without over-complicating the model with too many parameters. Estimation of model parameters and realizations of random effects can be accomplished by a combination of the Expected-Conditional-Maximization (ECM) algorithm and the Best Linear Unbiased Predictor (BLUP) procedure, adapted from Ng and McLachlan (2007). Finally, we present numerical simulations and case studies on a real insurance dataset.\nCindy Zhang\nFighting Noise with Noise: Causal Inference with Many Candidate Instruments\nInstrumental variable methods provide useful tools for inferring causal effects in the presence of unmeasured confounding. To apply these methods with large-scale data sets, a major challenge is to find valid instruments from a possibly large candidate set. In practice, most of the candidate instruments are often not relevant for studying a particular exposure of interest. Moreover, not all relevant candidate instruments are valid as they may directly influence the outcome of interest. We propose a data-driven method for causal inference with many candidate instruments that addresses these two challenges simultaneously. A key component of our proposal is a novel resampling method that constructs pseudo variables to remove irrelevant candidate instruments having spurious correlations with the exposure. To explore the potential benefit of our method, we examine the effect of obesity (as measured by BMI) on Health-Related Quality of Life (HRQL) using the data collected from the Wisconsin Longitudinal Study.\nYing Zhou\nThe Promises of Parallel Outcomes\nA key challenge in causal inference from observational studies is the identification of causal effects in the presence of unmeasured confounding. In this paper, we introduce a novel framework that leverages information in multiple parallel outcomes for causal identification with unmeasured confounding. Under a conditional independence structure among multiple parallel outcomes, we achieve nonparametric identification of causal effects with at least three parallel outcomes. Our identification results pave the road for causal effect estimation with multiple outcomes. In the Supplementary Material, we illustrate the promises of this framework by developing nonparametric estimating procedures in the discrete case, and evaluating their finite sample performance through numerical studies.\nPreprint: https://arxiv.org/pdf/2012.05849.pdf\n\n\n\n",
      "last_modified": "2022-04-11T12:35:28-04:00"
    },
    {
      "path": "index.html",
      "title": "Research Day 2022",
      "description": "Welcome to the website for the 2022 Statistics Student Research Day at the\n University of Toronto, May 25 2022.\n",
      "author": [],
      "contents": "\nAbout\nThe Statistics Student Research Day is an annual showcase and celebration of research excellence in the Department of Statistical Sciences at the University of Toronto. This year the event will be held virtually using Zoom on May 25, 2022, 9am to 4:45pm EDT.\nThe schedule can be found here. The event has three thematic blocks, corresponding to the main research areas in our department: Theoretical Statistics, Applied Statistics, and Math Finance and Actuarial Science. Each block will begin with an invited talk, followed by several talks by our own graduate students. The abstracts can be found here.\nEvent organizing committee\nGraduate student members: Emma Kroell (lead organizer), Blair Bilodeau, Michael Chong, Kathleen Miao, Marija Pejcinovska, Sabrina Sixta\nFaculty Members: Rohan Alexander, Jessica Gronsbell, Vianey Leos Barajas\n\n\n\n\n\n\n\n",
      "last_modified": "2022-04-11T12:35:29-04:00"
    },
    {
      "path": "registration.html",
      "title": "Registration",
      "author": [],
      "contents": "\nPlease register here (TBD) using Eventbrite.\n\n\n\n",
      "last_modified": "2022-04-11T12:35:29-04:00"
    },
    {
      "path": "schedule.html",
      "title": "Schedule",
      "description": "Below is a tentative schedule for the event. All times are in EDT.",
      "author": [],
      "contents": "\n9:05: Welcome\nBlock 1: Applied Statistics\nTime\nTitle\nSpeaker\n9:10 to 9:50\nInvited talk: Title TBA\nDr. Niamh Cahill\n9:50 to 10:00\nQ & A with Niamh\nDr. Niamh Cahill\n10:00 to 10:15\nTitle TBA\nTBA\n10:15 to 10:30\nTitle TBA\nTBA\n10:30 to 10:40\nQuestions with TBA\nTBA and TBA\n20 minute break\nBlock 2: Math Finance and Actuarial Science\nTime\nTitle\nSpeaker\n11:00 to 11:40\nInvited talk: Multiscale modeling for option pricing and optimal execution\nDr. Yuri Saporito\n11:40 to 11:50\nQ & A with Yuri\nDr. Yuri Saporito\n11:50 to 12:05\nModelling Heterogeneous Risks with Random Effects in the Mixture-of-Experts Model\nSpark Tseung\n12:05 to 12:20\nTitle TBA\nTBA\n12:20 to 12:30\nQuestions with Spark & TBA\nSpark Tseung and TBA\nLUNCH: 75 minutes\nBlock 3: Theoretical Statistics\nTime\nTitle\nSpeaker\n1:45 to 2:25\nInvited talk: Benign Overfitting without Linearity\nDr. Spencer Frei\n2:25 to 2:35\nQ & A with Spencer\nDr. Spencer Frei\n2:35 to 2:50\nFighting Noise with Noise: Causal Inference with Many Candidate Instruments\nCindy Zhang\n2:50 to 3:05\nTitle TBA\nTBA\n3:05 to 3:15\nQuestions with Cindy and TBA\nCindy Zhang and TBA\n3:15 to 3:25\nBREAK\n\n3:25 to 3:40\nConvergence rate bounds for iterative random functions using one-shot coupling\nSabrina Sixta\n3:40 to 3:55\nTitle TBA\nTBA\n3:55 to 4:05\nQuestions with Sabrina & TBA\nSabrina Sixta and TBA\n4:05 to 4:20\nTitle TBA\nTBA\n4:20 to 4:35\nThe Promises of Parallel Outcomes\nYing Zhou\n4:35 to 4:45:\nQuestions with TBA & Ying\nTBA and Ying Zhou\n\n\n\n",
      "last_modified": "2022-04-11T12:35:29-04:00"
    }
  ],
  "collections": []
}
